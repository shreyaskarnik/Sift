{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Sift — Fine-Tune EmbeddingGemma on Google Colab\n\nTrain your personal feed-scoring model on a free T4 GPU, then download the ONNX model for use with the Sift Chrome extension.\n\n## Prerequisites\n\n**Accept the EmbeddingGemma license** on HuggingFace before running:\nhttps://huggingface.co/google/embeddinggemma-300m\n\nYou'll also need an **HF token** (with read access) — create one at https://huggingface.co/settings/tokens and paste it in the Configuration cell below.\n\n## Quick Start\n1. **Set GPU runtime:** `Runtime → Change runtime type → T4 GPU`\n2. **Run all cells:** `Runtime → Run all`\n3. **Upload** your exported CSV when prompted\n4. **Download** the resulting ONNX zip at the end\n\nNo local setup required — everything runs in this notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (torch is pre-installed by Colab with CUDA)\n",
    "!pip install -q \\\n",
    "    \"sentence-transformers>=3.0\" \\\n",
    "    \"transformers>=4.56.0\" \\\n",
    "    \"datasets>=2.18\" \\\n",
    "    \"accelerate>=0.30\" \\\n",
    "    \"huggingface-hub>=0.23\" \\\n",
    "    \"optimum[exporters,onnxruntime]>=1.21\" \\\n",
    "    \"onnx>=1.16\" \\\n",
    "    \"onnxruntime>=1.18\" \\\n",
    "    \"onnx-ir>=0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\nif torch.cuda.is_available():\n    DEVICE = \"cuda\"\n    gpu_name = torch.cuda.get_device_name(0)\n    try:\n        vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n        print(f\"GPU: {gpu_name} ({vram_gb:.1f} GB VRAM)\")\n    except Exception:\n        print(f\"GPU: {gpu_name}\")\nelse:\n    DEVICE = \"cpu\"\n    print(\"WARNING: No GPU detected. Training will be very slow.\")\n    print(\"Go to Runtime → Change runtime type → T4 GPU\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "def load_csv(content: str) -> list[list[str]]:\n",
    "    \"\"\"Load Anchor,Positive,Negative triplets from CSV string.\"\"\"\n",
    "    triplets = []\n",
    "    reader = csv.reader(io.StringIO(content))\n",
    "\n",
    "    try:\n",
    "        first_row = next(reader)\n",
    "    except StopIteration:\n",
    "        return triplets\n",
    "\n",
    "    def maybe_append_triplet(row: list[str]) -> None:\n",
    "        if len(row) < 3:\n",
    "            return\n",
    "        anchor = row[0].strip()\n",
    "        positive = row[1].strip()\n",
    "        negative = row[2].strip()\n",
    "        if anchor and positive and negative:\n",
    "            triplets.append([anchor, positive, negative])\n",
    "\n",
    "    is_header = (\n",
    "        len(first_row) >= 3\n",
    "        and first_row[0].strip().lower() == \"anchor\"\n",
    "        and first_row[1].strip().lower() == \"positive\"\n",
    "        and first_row[2].strip().lower() == \"negative\"\n",
    "    )\n",
    "    if not is_header:\n",
    "        maybe_append_triplet(first_row)\n",
    "\n",
    "    for row in reader:\n",
    "        maybe_append_triplet(row)\n",
    "    return triplets\n",
    "\n",
    "\n",
    "uploaded = files.upload()\n",
    "csv_filename = list(uploaded.keys())[0]\n",
    "csv_content = uploaded[csv_filename].decode(\"utf-8\")\n",
    "triplets = load_csv(csv_content)\n",
    "\n",
    "print(f\"\\nLoaded {len(triplets)} triplets from {csv_filename}\")\n",
    "\n",
    "# Summary by anchor\n",
    "from collections import Counter\n",
    "anchor_counts = Counter(t[0] for t in triplets)\n",
    "for anchor, count in anchor_counts.most_common():\n",
    "    print(f\"  {anchor}: {count} triplets\")\n",
    "\n",
    "if triplets:\n",
    "    print(f\"\\nExample: {triplets[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Edit the values below before training. The defaults work well for most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Model ---\nMODEL_NAME = \"google/embeddinggemma-300m\"\nTASK_NAME = \"Classification\"\n\n# --- Training ---\nEPOCHS = 4\nLEARNING_RATE = 2e-5\nBATCH_SIZE = 4  # T4 16GB handles 4; reduce to 1 if you hit OOM\n\n# --- Held-out evaluation ---\nHELDOUT_FRACTION = 0.15  # set to 0 to use all data for training\nSEED = 42\n\n# --- Output ---\nFINETUNED_DIR = \"sift-finetuned\"\nONNX_DIR = \"sift-finetuned_onnx_transformersjs\"\n\n# --- HuggingFace ---\nHF_TOKEN = \"\"   # REQUIRED: needed to download gated model (read access)\nHF_REPO = \"\"    # e.g. \"yourname/sift-finetuned\" — leave empty to skip upload"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import random\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\n\nfrom datasets import Dataset\nfrom sentence_transformers import SentenceTransformer, util\nfrom sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom transformers import TrainerCallback\nfrom huggingface_hub import HfApi, login, model_info, metadata_update\n\n\n# --- Held-Out Evaluation ---\n\n@dataclass\nclass HeldOutItem:\n    text: str\n    is_positive: bool\n    baseline_score: float = 0.0\n\n@dataclass\nclass AnchorHeldOutGroup:\n    anchor: str\n    items: list[HeldOutItem] = field(default_factory=list)\n\n\ndef _normalize_text(text: str) -> str:\n    \"\"\"Lowercase + collapse whitespace for near-duplicate detection.\"\"\"\n    return \" \".join(text.lower().split())\n\n\ndef split_held_out(\n    triplets: list[list[str]],\n    fraction: float = 0.15,\n    min_anchor_triplets: int = 4,\n    seed: int = 42,\n) -> tuple[list[list[str]], list[AnchorHeldOutGroup]]:\n    \"\"\"Split triplets into train set and per-anchor held-out groups.\"\"\"\n    if fraction == 0:\n        return list(triplets), []\n\n    rng = random.Random(seed)\n\n    by_anchor: dict[str, list[list[str]]] = defaultdict(list)\n    for t in triplets:\n        by_anchor[t[0]].append(t)\n\n    train_triplets: list[list[str]] = []\n    held_out_groups: list[AnchorHeldOutGroup] = []\n\n    for anchor, rows in by_anchor.items():\n        if len(rows) < min_anchor_triplets:\n            train_triplets.extend(rows)\n            continue\n\n        shuffled = rows[:]\n        rng.shuffle(shuffled)\n        n_held = max(1, round(len(rows) * fraction))\n        held_rows = shuffled[:n_held]\n        train_part = shuffled[n_held:]\n        train_triplets.extend(train_part)\n\n        train_norms: set[str] = set()\n        for row in train_part:\n            train_norms.add(_normalize_text(row[1]))\n            train_norms.add(_normalize_text(row[2]))\n\n        seen_norm: set[str] = set()\n        items: list[HeldOutItem] = []\n        candidates: list[tuple[str, bool, str]] = []\n        for row in held_rows:\n            candidates.append((row[1], True, _normalize_text(row[1])))\n            candidates.append((row[2], False, _normalize_text(row[2])))\n\n        for text, is_pos, norm in candidates:\n            if norm not in seen_norm and norm not in train_norms:\n                seen_norm.add(norm)\n                items.append(HeldOutItem(text=text, is_positive=is_pos))\n\n        has_pos = any(i.is_positive for i in items)\n        has_neg = any(not i.is_positive for i in items)\n\n        if not has_pos:\n            for text, is_pos, norm in candidates:\n                if is_pos and norm not in seen_norm:\n                    seen_norm.add(norm)\n                    items.append(HeldOutItem(text=text, is_positive=True))\n                    break\n\n        if not has_neg:\n            for text, is_pos, norm in candidates:\n                if (not is_pos) and norm not in seen_norm:\n                    seen_norm.add(norm)\n                    items.append(HeldOutItem(text=text, is_positive=False))\n                    break\n\n        if items:\n            held_out_groups.append(AnchorHeldOutGroup(anchor=anchor, items=items))\n\n    return train_triplets, held_out_groups\n\n\n# --- Scoring & Formatting ---\n\ndef score_held_out_items(\n    model: SentenceTransformer,\n    groups: list[AnchorHeldOutGroup],\n    task_name: str,\n) -> dict[str, list[float]]:\n    \"\"\"Score each held-out item against its anchor.\"\"\"\n    results: dict[str, list[float]] = {}\n    for g in groups:\n        anchor_emb = model.encode(g.anchor, prompt_name=task_name)\n        texts = [item.text for item in g.items]\n        text_embs = model.encode(texts, prompt_name=task_name)\n        sims = util.cos_sim(anchor_emb, text_embs)[0].tolist()\n        results[g.anchor] = sims\n    return results\n\n\ndef format_taste_table(\n    groups: list[AnchorHeldOutGroup],\n    scores: dict[str, list[float]],\n    header: str,\n    show_baseline_delta: bool = False,\n) -> str:\n    \"\"\"Format a per-epoch taste table with optional delta from baseline.\"\"\"\n    lines = [f\"\\n=== Taste Check ({header}) {'=' * max(1, 45 - len(header))}\"]\n    for g in groups:\n        s = scores[g.anchor]\n        lines.append(f\"\\nAnchor: {g.anchor} ({len(g.items)} items)\")\n        pos_scores, neg_scores = [], []\n        for item, score in zip(g.items, s):\n            tag = \"+\" if item.is_positive else \"-\"\n            label = item.text[:50]\n            delta_str = \"\"\n            if show_baseline_delta:\n                delta = score - item.baseline_score\n                delta_str = f\"  ({delta:+.2f})\"\n            lines.append(f\"  {tag} \\\"{label}\\\"{' ' * max(1, 55 - len(label))}{score:.2f}{delta_str}\")\n            (pos_scores if item.is_positive else neg_scores).append(score)\n\n        avg_p = sum(pos_scores) / len(pos_scores) if pos_scores else 0\n        avg_n = sum(neg_scores) / len(neg_scores) if neg_scores else 0\n        gap = avg_p - avg_n\n        n_pairs = len(pos_scores) * len(neg_scores)\n        n_correct = sum(1 for ps in pos_scores for ns in neg_scores if ps > ns)\n        pair_pct = (n_correct / n_pairs * 100) if n_pairs else 0\n        gap_str = f\"  gap: {gap:.2f}\"\n        pair_str = f\"  pos>neg: {pair_pct:.0f}%\"\n        if show_baseline_delta:\n            bp = [it.baseline_score for it in g.items if it.is_positive]\n            bn = [it.baseline_score for it in g.items if not it.is_positive]\n            old_gap = (sum(bp) / len(bp) if bp else 0) - (sum(bn) / len(bn) if bn else 0)\n            gap_str += f\"  (was {old_gap:.2f})\"\n            base_correct = sum(1 for ps in bp for ns in bn if ps > ns)\n            base_pairs = len(bp) * len(bn)\n            base_pct = (base_correct / base_pairs * 100) if base_pairs else 0\n            pair_str += f\" (was {base_pct:.0f}%)\"\n        lines.append(f\"  avg +: {avg_p:.2f}  avg -: {avg_n:.2f}{gap_str}{pair_str}\")\n    return \"\\n\".join(lines)\n\n\ndef format_taste_final(\n    groups: list[AnchorHeldOutGroup],\n    final_scores: dict[str, list[float]],\n) -> str:\n    \"\"\"Format the before→after final summary.\"\"\"\n    lines = [f\"\\n=== Taste Check -- Final {'=' * 30}\"]\n    for g in groups:\n        s = final_scores[g.anchor]\n        lines.append(f\"\\nAnchor: {g.anchor}\")\n        lines.append(f\"  {'':55s} Before -> After\")\n        pos_before, pos_after, neg_before, neg_after = [], [], [], []\n        for item, score in zip(g.items, s):\n            tag = \"+\" if item.is_positive else \"-\"\n            label = item.text[:50]\n            delta = score - item.baseline_score\n            lines.append(\n                f\"  {tag} \\\"{label}\\\"{' ' * max(1, 55 - len(label))}\"\n                f\"{item.baseline_score:.2f}  ->  {score:.2f}  ({delta:+.2f})\"\n            )\n            if item.is_positive:\n                pos_before.append(item.baseline_score)\n                pos_after.append(score)\n            else:\n                neg_before.append(item.baseline_score)\n                neg_after.append(score)\n\n        avg_pb = sum(pos_before) / len(pos_before) if pos_before else 0\n        avg_pa = sum(pos_after) / len(pos_after) if pos_after else 0\n        avg_nb = sum(neg_before) / len(neg_before) if neg_before else 0\n        avg_na = sum(neg_after) / len(neg_after) if neg_after else 0\n        gap_b = avg_pb - avg_nb\n        gap_a = avg_pa - avg_na\n        n_pairs = len(pos_before) * len(neg_before)\n        pct_b = (sum(1 for p in pos_before for n in neg_before if p > n) / n_pairs * 100) if n_pairs else 0\n        pct_a = (sum(1 for p in pos_after for n in neg_after if p > n) / n_pairs * 100) if n_pairs else 0\n        lines.append(\n            f\"  avg +: {avg_pb:.2f} -> {avg_pa:.2f}  \"\n            f\"avg -: {avg_nb:.2f} -> {avg_na:.2f}  \"\n            f\"gap: {gap_b:.2f} -> {gap_a:.2f}  \"\n            f\"pos>neg: {pct_b:.0f}% -> {pct_a:.0f}%\"\n        )\n    return \"\\n\".join(lines)\n\n\nclass TasteTracker(TrainerCallback):\n    \"\"\"Scores held-out items at baseline and each epoch to track taste alignment.\"\"\"\n\n    def __init__(\n        self,\n        model: SentenceTransformer,\n        groups: list[AnchorHeldOutGroup],\n        task_name: str,\n    ):\n        self.model = model\n        self.groups = groups\n        self.task_name = task_name\n        self.final_scores: dict[str, list[float]] = {}\n\n    def on_train_begin(self, args, state, control, **kwargs):\n        scores = score_held_out_items(self.model, self.groups, self.task_name)\n        for g in self.groups:\n            for item, s in zip(g.items, scores[g.anchor]):\n                item.baseline_score = s\n        if state.is_world_process_zero:\n            print(format_taste_table(self.groups, scores, \"baseline\"))\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        epoch = int(state.epoch)\n        scores = score_held_out_items(self.model, self.groups, self.task_name)\n        self.final_scores = scores\n        if state.is_world_process_zero:\n            print(format_taste_table(self.groups, scores, f\"epoch {epoch}\", show_baseline_delta=True))\n\n    def get_final_summary(self) -> str:\n        if not self.final_scores:\n            return \"\"\n        return format_taste_final(self.groups, self.final_scores)\n\n\n# --- Split held-out ---\n\ntrain_triplets, held_out_groups = split_held_out(\n    triplets,\n    fraction=HELDOUT_FRACTION,\n    min_anchor_triplets=4,\n    seed=SEED,\n)\n\nheld_out_count = sum(len(g.items) for g in held_out_groups)\nif held_out_groups:\n    anchors = \", \".join(g.anchor for g in held_out_groups)\n    print(f\"Split: {len(train_triplets)} train, {held_out_count} held-out items across {len(held_out_groups)} anchor(s) [{anchors}]\")\nelse:\n    print(f\"All {len(train_triplets)} triplets used for training (too few per anchor to split)\")\n\nassert len(train_triplets) >= 2, (\n    f\"Need at least 2 training triplets, got {len(train_triplets)}. Collect more labels!\"\n)\n\n# --- Load model ---\n\nprint(f\"\\nLoading {MODEL_NAME} on {DEVICE}...\")\n\nif HF_TOKEN:\n    login(token=HF_TOKEN, add_to_git_credential=True)\n    model = SentenceTransformer(MODEL_NAME, device=DEVICE, token=HF_TOKEN)\nelse:\n    model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n\nprint(f\"Model loaded on {model.device}\")\n\n# --- Train ---\n\ndata_as_dicts = [\n    {\"anchor\": row[0], \"positive\": row[1], \"negative\": row[2]}\n    for row in train_triplets\n]\ntrain_dataset = Dataset.from_list(data_as_dicts)\nloss = MultipleNegativesRankingLoss(model)\n\nprompts = getattr(model, 'prompts', {}).get(TASK_NAME)\n\ncallbacks = []\ntaste_tracker = None\nif held_out_groups:\n    taste_tracker = TasteTracker(model, held_out_groups, TASK_NAME)\n    callbacks.append(taste_tracker)\n\nis_cuda = DEVICE == \"cuda\"\n\ntraining_args = SentenceTransformerTrainingArguments(\n    output_dir=FINETUNED_DIR,\n    prompts=prompts,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    learning_rate=LEARNING_RATE,\n    warmup_ratio=0.1,\n    logging_steps=train_dataset.num_rows,\n    report_to=\"none\",\n    save_strategy=\"no\",\n    dataloader_pin_memory=is_cuda,\n    fp16=is_cuda,\n)\n\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    loss=loss,\n    callbacks=callbacks,\n)\n\nprint(f\"\\nTraining: {EPOCHS} epochs, lr={LEARNING_RATE}, batch_size={BATCH_SIZE}\")\ntrainer.train()\n\nprint(\"Training finished. Saving model...\")\ntrainer.save_model()\nprint(f\"Model saved to: {FINETUNED_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display taste eval results (before → after)\n",
    "if taste_tracker:\n",
    "    print(taste_tracker.get_final_summary())\n",
    "else:\n",
    "    print(\"No held-out evaluation (HELDOUT_FRACTION was 0 or too few triplets per anchor).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ONNX Conversion\n\nConverts the fine-tuned model to ONNX format for use with Transformers.js in the Chrome extension.\nProduces four variants: fp32, int8, q4 (WASM), and q4 no_gather (WebGPU-compatible)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import logging\nimport shutil\nimport warnings\nfrom pathlib import Path\nfrom optimum.exporters.onnx import main_export\n\nmodel_dir = Path(FINETUNED_DIR)\noutput_dir = Path(ONNX_DIR)\n\nprint(\"--- ONNX Conversion ---\")\nprint(f\"Exporting {model_dir} → {output_dir}...\")\n\ntry:\n    from torch.jit import TracerWarning\nexcept Exception:\n    TracerWarning = UserWarning\n\nexport_loggers = (\"transformers\", \"optimum\", \"torch.onnx\", \"onnxruntime\")\nprev_logger_levels = {}\nfor name in export_loggers:\n    logger = logging.getLogger(name)\n    prev_logger_levels[name] = logger.level\n    logger.setLevel(logging.ERROR)\ntry:\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", message=\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n        warnings.filterwarnings(\n            \"ignore\",\n            message=r\"The tokenizer you are loading from .*incorrect regex pattern.*\",\n        )\n        warnings.filterwarnings(\"ignore\", category=TracerWarning)\n        warnings.filterwarnings(\n            \"ignore\",\n            message=r\"Exporting aten::index operator of advanced indexing.*\",\n            category=UserWarning,\n        )\n        main_export(\n            model_name_or_path=str(model_dir),\n            output=output_dir,\n            task=\"feature-extraction\",\n            device=\"cpu\",\n            dtype=\"fp32\",\n            library_name=\"sentence_transformers\",\n            do_validation=False,\n        )\nfinally:\n    for name, level in prev_logger_levels.items():\n        logging.getLogger(name).setLevel(level)\n\n# optimum puts model.onnx at root; Transformers.js expects onnx/ subdirectory\nonnx_subdir = output_dir / \"onnx\"\nonnx_subdir.mkdir(exist_ok=True)\nroot_onnx = output_dir / \"model.onnx\"\nonnx_path = onnx_subdir / \"model.onnx\"\nif root_onnx.exists():\n    shutil.move(str(root_onnx), str(onnx_path))\n\nsize_mb = onnx_path.stat().st_size / (1024 * 1024)\nprint(f\"ONNX model (fp32): {size_mb:.1f} MB\")\n\n# INT8 dynamic quantization\nprev_disable = logging.root.manager.disable\nlogging.disable(logging.WARNING)\n\ntry:\n    from onnxruntime.quantization import quantize_dynamic, QuantType\n\n    quant_path = onnx_subdir / \"model_quantized.onnx\"\n    print(\"Quantizing to INT8...\")\n    quantize_dynamic(str(onnx_path), str(quant_path), weight_type=QuantType.QInt8)\n    print(f\"INT8 model: {quant_path.stat().st_size / (1024*1024):.1f} MB\")\nexcept Exception as e:\n    print(f\"INT8 quantization failed (non-critical): {e}\")\n\n# 4-bit block quantization\ntry:\n    import onnx\n    from onnxruntime.quantization.matmul_nbits_quantizer import MatMulNBitsQuantizer\n\n    print(\"Quantizing to Q4...\")\n    model_proto = onnx.load(str(onnx_path))\n    quant = MatMulNBitsQuantizer(\n        model_proto, block_size=32, is_symmetric=True, accuracy_level=4,\n    )\n    quant.process()\n    q4_path = onnx_subdir / \"model_q4.onnx\"\n    onnx.save(quant.model.model, str(q4_path))\n    print(f\"Q4 model: {q4_path.stat().st_size / (1024*1024):.1f} MB\")\n\n    # WebGPU-compatible variant: strip GatherElements ops if present\n    no_gather_path = onnx_subdir / \"model_no_gather_q4.onnx\"\n    q4_model = onnx.load(str(q4_path))\n    gather_nodes = [n for n in q4_model.graph.node if n.op_type == \"GatherElements\"]\n    if not gather_nodes:\n        shutil.copy2(str(q4_path), str(no_gather_path))\n        print(f\"Q4 no_gather (WebGPU): copied (no GatherElements ops found)\")\n    else:\n        for node in gather_nodes:\n            node.op_type = \"Gather\"\n        onnx.save(q4_model, str(no_gather_path))\n        print(f\"Q4 no_gather (WebGPU): replaced {len(gather_nodes)} GatherElements → Gather\")\nexcept Exception as e:\n    print(f\"Q4 quantization failed (non-critical): {e}\")\n\nlogging.disable(prev_disable)\n\nprint(f\"\\nTransformers.js model ready at: {output_dir}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "zip_name = \"sift-onnx-model\"\n",
    "shutil.make_archive(zip_name, \"zip\", str(output_dir))\n",
    "print(f\"Created {zip_name}.zip\")\n",
    "files.download(f\"{zip_name}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Push to HuggingFace Hub\n",
    "\n",
    "Set `HF_REPO` and `HF_TOKEN` in the Configuration cell above, then run this cell.\n",
    "The model must be public for the Sift extension to load it (HF auth is not supported browser-side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not HF_REPO:\n    print(\"Skipping Hub upload: HF_REPO not set in Configuration cell.\")\nelse:\n    if not HF_TOKEN:\n        print(\"Error: HF_TOKEN is required to push to Hub.\")\n    else:\n        login(token=HF_TOKEN)\n        api = HfApi(token=HF_TOKEN)\n\n        requested = HF_REPO.strip().strip(\"/\")\n        if \"/\" in requested:\n            repo_id = requested\n        else:\n            user_info = api.whoami()\n            repo_id = f\"{user_info['name']}/{requested}\"\n\n        print(f\"Uploading to: {repo_id}\")\n        api.create_repo(repo_id=repo_id, exist_ok=True)\n        url = api.upload_folder(\n            folder_path=str(output_dir),\n            repo_id=repo_id,\n            repo_type=\"model\",\n        )\n\n        info = model_info(repo_id=repo_id, token=HF_TOKEN)\n        tags = list((info.card_data.tags if info.card_data else []) or [])\n        if \"embeddinggemma-tuning-lab\" not in tags:\n            tags.append(\"embeddinggemma-tuning-lab\")\n            metadata_update(\n                repo_id=repo_id,\n                metadata={\"tags\": tags},\n                overwrite=True,\n                token=HF_TOKEN,\n            )\n\n        print(f\"\\nSuccess! Model published at: {url}\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}